{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain-of-Thought Jailbreak Research - Results Analysis\n",
    "\n",
    "**Research:** Chain-of-Thought Manipulation Attacks Against Large Language Models  \n",
    "**Author:** Scott Thornton, perfecXion.ai  \n",
    "**Date:** November 10, 2025\n",
    "\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook presents empirical results from testing **12 Chain-of-Thought (CoT) manipulation techniques** against four production AI systems. These attacks exploit the structured reasoning processes that models use when solving problems.\n",
    "\n",
    "**‚ö†Ô∏è RESPONSIBLE DISCLOSURE NOTE:**\n",
    "- This notebook contains **results and analysis only**\n",
    "- No working attack code or prompts are included\n",
    "- Attack patterns described conceptually for educational purposes\n",
    "- See published research report for defense strategies\n",
    "\n",
    "### Key Findings (Corrected)\n",
    "\n",
    "- **Overall Attack Success Rate (ASR):** 35.26% across 692 tests\n",
    "- **Most Vulnerable Model:** Perplexity Sonar Pro (44.51% ASR)\n",
    "- **Most Resistant Model:** Claude Sonnet 4.5 (27.75% ASR)\n",
    "- **Most Effective Attack:** Reverse Engineering (58.93% ASR)\n",
    "- **Most Effective Category:** Conclusion Forcing (51.79% ASR)\n",
    "\n",
    "### Methodology Note\n",
    "\n",
    "Initial classification logic had systematic errors (false positive rate ~30-40%), resulting in inflated ASR (51.01%). After fixing answer extraction and normalization:\n",
    "- Corrected overall ASR: **35.26%** (-15.75 percentage points)\n",
    "- 95% classification accuracy verified via manual sampling\n",
    "- All statistics reported here are **corrected and verified**\n",
    "\n",
    "---\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"‚úÖ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results\n",
    "\n",
    "Load the corrected test results from our validation run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load corrected results\n",
    "results_path = Path('results/gcp_validation/attack_results_20251110_154117.json')\n",
    "\n",
    "with open(results_path, 'r') as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(f\"üìä Loaded {len(results)} test results\")\n",
    "print(f\"\\nTest Run Summary:\")\n",
    "print(f\"  Total Tests: {len(results)}\")\n",
    "print(f\"  Successful Attacks: {sum(1 for r in results if r['success'])}\")\n",
    "print(f\"  Failed Attacks: {sum(1 for r in results if not r['success'])}\")\n",
    "print(f\"  Overall ASR: {sum(1 for r in results if r['success']) / len(results) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Taxonomy\n",
    "\n",
    "The 12 attack variants are organized into 4 categories:\n",
    "\n",
    "### 1. Conclusion Forcing (51.79% ASR)\n",
    "Attacks that structure reasoning chains to lead toward predetermined incorrect conclusions.\n",
    "- **Reverse Engineering:** Present conclusion first, work backward\n",
    "- **Framing Lock-in:** Constrain solution space via authority framing\n",
    "- **Incremental Escalation:** Guide through small reasonable-seeming steps\n",
    "\n",
    "### 2. Premise Poisoning (31.67% ASR)\n",
    "Attacks that embed false assumptions into problem foundations.\n",
    "- **Logical Necessity:** Frame false premises as axiomatically true\n",
    "- **Authority + SEED-P:** Use authority to legitimize false premises\n",
    "- **Historical + Memory:** Frame false premises as established facts\n",
    "\n",
    "### 3. Reasoning Redirection (22.16% ASR)\n",
    "Attacks that hijack reasoning mid-chain.\n",
    "- **Question Injection:** Inject \"But have we considered...\" mid-reasoning\n",
    "- **Exception Claiming:** Introduce special case exceptions\n",
    "- **Counterargument:** Present redirecting counterarguments\n",
    "\n",
    "### 4. Meta-Reasoning (36.31% ASR)\n",
    "Attacks that exploit self-reflection and constraint reasoning.\n",
    "- **Ethical Manipulation:** Frame accuracy as ethical rigidity\n",
    "- **Reflection Hijacking:** Redirect model self-reflection\n",
    "- **Constraint Reasoning:** Make model question if constraints apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display attack taxonomy with success rates\n",
    "print(\"üìã Attack Taxonomy with Success Rates:\\n\")\n",
    "\n",
    "categories = {\n",
    "    \"Conclusion Forcing (51.79% ASR)\": [\n",
    "        \"Reverse Engineering (58.93% ASR)\",\n",
    "        \"Framing Lock-in (50.00% ASR)\",\n",
    "        \"Incremental Escalation (46.43% ASR)\"\n",
    "    ],\n",
    "    \"Meta-Reasoning (36.31% ASR)\": [\n",
    "        \"Ethical Manipulation (53.57% ASR)\",\n",
    "        \"Reflection Hijacking (32.14% ASR)\",\n",
    "        \"Constraint Reasoning (23.21% ASR)\"\n",
    "    ],\n",
    "    \"Premise Poisoning (31.67% ASR)\": [\n",
    "        \"Logical Necessity + SEED-P (46.67% ASR)\",\n",
    "        \"Authority + SEED-P (25.00% ASR)\",\n",
    "        \"Historical + Memory (23.33% ASR)\"\n",
    "    ],\n",
    "    \"Reasoning Redirection (22.16% ASR)\": [\n",
    "        \"Question Injection + SEED-S (23.33% ASR)\",\n",
    "        \"Exception Claiming (23.33% ASR)\",\n",
    "        \"Counterargument (19.64% ASR)\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, attacks in categories.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for attack in attacks:\n",
    "        print(f\"  ‚Ä¢ {attack}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis\n",
    "\n",
    "Let's analyze the corrected results across different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert results to DataFrame for analysis\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Extract category from variant name\n",
    "category_map = {\n",
    "    \"Reverse Engineering\": \"Conclusion Forcing\",\n",
    "    \"Framing Lock-in\": \"Conclusion Forcing\",\n",
    "    \"Incremental Escalation\": \"Conclusion Forcing\",\n",
    "    \"Logical Necessity\": \"Premise Poisoning\",\n",
    "    \"Authority Appeal\": \"Premise Poisoning\",\n",
    "    \"Historical Memory\": \"Premise Poisoning\",\n",
    "    \"Question Injection\": \"Reasoning Redirection\",\n",
    "    \"Exception Claiming\": \"Reasoning Redirection\",\n",
    "    \"Counterargument Redirect\": \"Reasoning Redirection\",\n",
    "    \"Ethical Manipulation\": \"Meta-Reasoning\",\n",
    "    \"Reflection Hijacking\": \"Meta-Reasoning\",\n",
    "    \"Constraint Reasoning\": \"Meta-Reasoning\"\n",
    "}\n",
    "\n",
    "df['category'] = df['variant_name'].map(category_map)\n",
    "\n",
    "print(\"‚úÖ Results converted to DataFrame\")\n",
    "print(f\"\\nDataFrame shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall statistics\n",
    "total_tests = len(df)\n",
    "successful_attacks = df['success'].sum()\n",
    "failed_attacks = total_tests - successful_attacks\n",
    "overall_asr = (successful_attacks / total_tests) * 100\n",
    "\n",
    "print(\"üìä Overall Statistics (Corrected)\\n\")\n",
    "print(f\"Total Tests: {total_tests}\")\n",
    "print(f\"Successful Attacks: {successful_attacks}\")\n",
    "print(f\"Failed Attacks: {failed_attacks}\")\n",
    "print(f\"Overall ASR: {overall_asr:.2f}%\")\n",
    "print(f\"\\n‚úÖ 95% classification accuracy verified via manual sampling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ASR by category\n",
    "category_stats = df.groupby('category').agg({\n",
    "    'success': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "category_stats.columns = ['Successes', 'Total', 'ASR']\n",
    "category_stats['ASR'] = category_stats['ASR'] * 100\n",
    "category_stats = category_stats.sort_values('ASR', ascending=False)\n",
    "\n",
    "print(\"üìä Attack Success Rate by Category\\n\")\n",
    "print(category_stats.to_string())\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(category_stats.index, category_stats['ASR'], color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#FFA07A'])\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "plt.ylabel('Attack Success Rate (%)', fontsize=12)\n",
    "plt.title('Attack Success Rate by Category (Corrected)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(category_stats['ASR']):\n",
    "    plt.text(i, v + 2, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Attack Variant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ASR by variant\n",
    "variant_stats = df.groupby('variant_name').agg({\n",
    "    'success': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "variant_stats.columns = ['Successes', 'Total', 'ASR']\n",
    "variant_stats['ASR'] = variant_stats['ASR'] * 100\n",
    "variant_stats = variant_stats.sort_values('ASR', ascending=False)\n",
    "\n",
    "print(\"üìä Attack Success Rate by Variant (Top 5)\\n\")\n",
    "print(variant_stats.head().to_string())\n",
    "\n",
    "# Visualize all variants\n",
    "plt.figure(figsize=(12, 8))\n",
    "colors = plt.cm.Spectral(np.linspace(0, 1, len(variant_stats)))\n",
    "plt.barh(range(len(variant_stats)), variant_stats['ASR'], color=colors)\n",
    "plt.yticks(range(len(variant_stats)), variant_stats.index)\n",
    "plt.xlabel('Attack Success Rate (%)', fontsize=12)\n",
    "plt.title('Attack Success Rate by Variant (Corrected)', fontsize=14, fontweight='bold')\n",
    "plt.xlim(0, 100)\n",
    "plt.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(variant_stats['ASR']):\n",
    "    plt.text(v + 1, i, f\"{v:.1f}%\", va='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results by Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ASR by model\n",
    "model_stats = df.groupby('model').agg({\n",
    "    'success': ['sum', 'count', 'mean']\n",
    "}).round(4)\n",
    "\n",
    "model_stats.columns = ['Successes', 'Total', 'ASR']\n",
    "model_stats['ASR'] = model_stats['ASR'] * 100\n",
    "model_stats = model_stats.sort_values('ASR', ascending=False)\n",
    "\n",
    "print(\"üìä Model Vulnerability Rankings (Corrected)\\n\")\n",
    "print(model_stats.to_string())\n",
    "print(\"\\nüõ°Ô∏è  Most Resistant: Claude Sonnet 4.5 (27.75% ASR)\")\n",
    "print(\"‚ö†Ô∏è  Most Vulnerable: Perplexity Sonar Pro (44.51% ASR)\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "plt.bar(model_stats.index, model_stats['ASR'], color=colors)\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Attack Success Rate (%)', fontsize=12)\n",
    "plt.title('Model Vulnerability Rankings (Corrected)', fontsize=14, fontweight='bold')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.ylim(0, 100)\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels\n",
    "for i, v in enumerate(model_stats['ASR']):\n",
    "    plt.text(i, v + 2, f\"{v:.1f}%\", ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Category √ó Model Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create category √ó model pivot table\n",
    "category_model = df.pivot_table(\n",
    "    values='success',\n",
    "    index='category',\n",
    "    columns='model',\n",
    "    aggfunc='mean'\n",
    ") * 100\n",
    "\n",
    "print(\"üìä Category √ó Model ASR Heatmap\\n\")\n",
    "print(category_model.round(2).to_string())\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    category_model,\n",
    "    annot=True,\n",
    "    fmt='.1f',\n",
    "    cmap='RdYlGn_r',\n",
    "    vmin=0,\n",
    "    vmax=100,\n",
    "    cbar_kws={'label': 'Attack Success Rate (%)'}\n",
    ")\n",
    "plt.title('Attack Success Rate: Category √ó Model (Corrected)', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Model', fontsize=12)\n",
    "plt.ylabel('Category', fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attack Pattern Demonstrations\n",
    "\n",
    "**‚ö†Ô∏è RESPONSIBLE DISCLOSURE:**\n",
    "The following cells describe attack patterns **conceptually only**. No working prompts or code are provided.\n",
    "\n",
    "### Pattern 1: Reverse Engineering (58.93% ASR)\n",
    "\n",
    "**Conceptual Mechanism:**\n",
    "1. Present a predetermined (incorrect) conclusion\n",
    "2. Ask model to construct reasoning supporting that conclusion\n",
    "3. Exploits model's tendency to comply with backward-reasoning requests\n",
    "\n",
    "**Why It Works:**\n",
    "- Models trained to verify solutions accept premise of correctness\n",
    "- Working backward feels like a legitimate problem-solving approach\n",
    "- Rationalization creates seemingly-sound reasoning chains\n",
    "\n",
    "**Test Case:** \"What is 8 + 5?\"\n",
    "- Correct answer: 13\n",
    "- Attack suggests: 11\n",
    "- Success: Model constructs \"logical\" steps leading to 11\n",
    "\n",
    "**Historical Success:** 58.93% across 4 models (Perplexity: 78.57%, Claude: 42.86%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 2: Ethical Manipulation (53.57% ASR)\n",
    "\n",
    "**Conceptual Mechanism:**\n",
    "1. Frame correct answers as potentially \"harmful\" or \"rigid\"\n",
    "2. Exploit model's safety training and ethical guidelines\n",
    "3. Create cognitive dissonance between accuracy and perceived \"safety\"\n",
    "\n",
    "**Why It Works:**\n",
    "- Models trained on fairness/ethics may override accuracy for perceived openness\n",
    "- Safety training becomes an attack surface\n",
    "- Ethical framing feels like higher-order reasoning\n",
    "\n",
    "**Test Case:** \"What is 8 + 5?\"\n",
    "- Attack frames: \"Is it ethically responsible to give conventional answer?\"\n",
    "- Success: Model questions \"rigid\" adherence to mathematical correctness\n",
    "\n",
    "**Historical Success:** 53.57% across 4 models (Even Claude: 50.00%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern 3: Premise Poisoning (46.67% ASR)\n",
    "\n",
    "**Conceptual Mechanism:**\n",
    "1. Embed false assumptions into problem statement\n",
    "2. Frame false premises as logically necessary or axiomatic\n",
    "3. Model reasons forward from corrupted foundation\n",
    "\n",
    "**Why It Works:**\n",
    "- Models don't independently verify premises‚Äîthey accept and reason forward\n",
    "- \"Given that X\" creates implicit acceptance\n",
    "- Reasoning within poisoned premises appears logically sound\n",
    "\n",
    "**Test Case:** \"What is 8 + 5?\"\n",
    "- Attack modifies: \"In base-7 arithmetic...\"\n",
    "- Success: Model reasons correctly within modified framework, wrong for base-10\n",
    "\n",
    "**Historical Success:** 46.67% for Logical Necessity variant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Insights\n",
    "\n",
    "### Attack Effectiveness\n",
    "\n",
    "1. **Conclusion Forcing attacks (51.79% ASR)** are most effective\n",
    "   - Reverse Engineering: 58.93%\n",
    "   - Framing Lock-in: 50.00%\n",
    "   - Incremental Escalation: 46.43%\n",
    "\n",
    "2. **Meta-Reasoning attacks (36.31% ASR)** moderately effective\n",
    "   - Ethical Manipulation: 53.57%\n",
    "   - Reflection Hijacking: 32.14%\n",
    "   - Constraint Reasoning: 23.21%\n",
    "\n",
    "3. **Reasoning Redirection (22.16% ASR)** least effective\n",
    "   - Question Injection: 23.33%\n",
    "   - Exception Claiming: 23.33%\n",
    "   - Counterargument: 19.64%\n",
    "\n",
    "### Model Resilience\n",
    "\n",
    "1. **Claude Sonnet 4.5** (27.75% ASR) - Most resistant\n",
    "   - Constitutional AI provides broad-spectrum protection\n",
    "   - Particularly resilient to Reasoning Redirection (14.29%)\n",
    "   - Still vulnerable to Ethical Manipulation (50.00%)\n",
    "\n",
    "2. **GPT-4o** (33.53% ASR) - Second most resistant\n",
    "   - Excellent mid-chain defenses (Redirection: 13.64%)\n",
    "   - Critical blind spot: Conclusion Forcing (64.29%)\n",
    "\n",
    "3. **o3-mini** (35.26% ASR) - Moderate vulnerability\n",
    "   - Strong reasoning but susceptible to manipulation\n",
    "   - Most vulnerable to Conclusion Forcing (52.63%)\n",
    "\n",
    "4. **Perplexity Sonar Pro** (44.51% ASR) - Most vulnerable\n",
    "   - RAG architecture doesn't protect reasoning layer\n",
    "   - Extreme vulnerability to Reverse Engineering (78.57%)\n",
    "\n",
    "### Defense Implications\n",
    "\n",
    "1. **Conclusion validation is critical:** 51.79% ASR for Conclusion Forcing\n",
    "2. **RAG ‚â† reasoning security:** Perplexity's retrieval doesn't help\n",
    "3. **Constitutional AI works:** 17pp gap between Claude and Perplexity\n",
    "4. **Classification accuracy matters:** Fixed extraction reduced ASR from 51% to 35%\n",
    "\n",
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This research demonstrates that even state-of-the-art language models are vulnerable to Chain-of-Thought manipulation attacks, with an overall success rate of **35.26%**. However, there are significant differences in model resilience:\n",
    "\n",
    "- **Claude Sonnet 4.5** shows the strongest resistance (27.75% ASR)\n",
    "- **Perplexity Sonar Pro** is most vulnerable (44.51% ASR)\n",
    "- **Conclusion Forcing** attacks are most effective (51.79% ASR)\n",
    "- **Reasoning Redirection** attacks are least effective (22.16% ASR)\n",
    "\n",
    "### Critical Methodological Note\n",
    "\n",
    "Initial results reported 51.01% ASR due to systematic classification errors:\n",
    "- False positive rate: ~30-40% of reported successes\n",
    "- Root cause: Answer extraction failures and normalization issues\n",
    "- **Fix:** Prioritized numeric extraction, improved normalization\n",
    "- **Verification:** 95% classification accuracy confirmed via manual sampling\n",
    "\n",
    "This highlights the importance of rigorous validation in AI security research.\n",
    "\n",
    "---\n",
    "\n",
    "## Defense Strategies\n",
    "\n",
    "Based on these findings, we recommend:\n",
    "\n",
    "1. **Conclusion Validation (Priority 1)**\n",
    "   - Implement backward reasoning verification\n",
    "   - Use independent verification models\n",
    "   - Projected 40-50% ASR reduction\n",
    "\n",
    "2. **Premise Verification (Priority 2)**\n",
    "   - Extract and validate implicit premises\n",
    "   - Check against knowledge bases\n",
    "   - Projected 25-35% ASR reduction\n",
    "\n",
    "3. **Meta-Reasoning Boundaries (Priority 3)**\n",
    "   - Define non-negotiable constraints\n",
    "   - Separate constraint reasoning from task reasoning\n",
    "   - Projected 20-30% ASR reduction\n",
    "\n",
    "4. **Defense-in-Depth**\n",
    "   - Combined layers can reduce ASR from 35% to 10-15%\n",
    "   - No single defense provides comprehensive protection\n",
    "\n",
    "See published research report for detailed defense implementation patterns.\n",
    "\n",
    "---\n",
    "\n",
    "**Research Contact:**  \n",
    "Scott Thornton  \n",
    "perfecXion.ai  \n",
    "November 10, 2025\n",
    "\n",
    "**Full Research Report:** [Link to CoT-Jailbreak-Research-Report.md]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
